version: '3.8'

services:
  mcp-graph:
    build: .
    container_name: mcp-graph-server
    environment:
      - DB_PATH=/app/data/ontology.db
      - SPARQL_ENDPOINT=https://dbpedia.org/sparql
      - LOG_LEVEL=info
      - EMBEDDING_BATCH_SIZE=32
    volumes:
      # Persistent database storage
      - ./data:/app/data
      # Optional: mount logs directory
      - ./logs:/app/logs
    restart: unless-stopped
    # MCP uses stdio, no port mapping needed
    stdin_open: true
    tty: true
    
  # CUDA-enabled service (requires nvidia-docker)
  mcp-graph-cuda:
    build:
      context: .
    container_name: mcp-graph-cuda
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - DB_PATH=/app/data/ontology.db
      - SPARQL_ENDPOINT=https://dbpedia.org/sparql
      - LOG_LEVEL=info
      - EMBEDDING_BATCH_SIZE=32
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    restart: unless-stopped
    stdin_open: true
    tty: true
    profiles:
      - cuda
    
  # Optional: Development service with hot reload
  mcp-graph-dev:
    build:
      context: .
      target: builder
    container_name: mcp-graph-dev
    environment:
      - DB_PATH=/app/data/ontology.db
      - SPARQL_ENDPOINT=https://dbpedia.org/sparql
      - LOG_LEVEL=debug
      - EMBEDDING_BATCH_SIZE=16
    volumes:
      - .:/app
      - /app/node_modules
      - ./data:/app/data
    working_dir: /app
    command: ["npm", "run", "start"]
    stdin_open: true
    tty: true
    profiles:
      - dev